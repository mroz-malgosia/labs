{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81b52832",
   "metadata": {},
   "source": [
    "# Data Engineer Take Home Exercise\n",
    "\n",
    "## Question\n",
    "### Data Prep\n",
    "\n",
    "Write a script to transform input CSV to desired output CSV and Parquet. \n",
    "\n",
    "You will find a CSV file in the files folder under `data.csv`. There are three steps to this part of the test. Each step concerns manipulating the values for a single field according to the step's requirements. The steps are as follows:\n",
    "\n",
    "**String cleaning** - The bio field contains text with arbitrary padding, spacing and line breaks. Normalize these values to a space-delimited string.\n",
    "\n",
    "**Code swap** - There is a supplementary CSV in the files folder under `state_abbreviations`. This \"data dictionary\" contains state abbreviations alongside state names. For the state field of the input CSV, replace each state abbreviation with its associated state name from the data dictionary.\n",
    "\n",
    "**Date offset** - The start_date field contains data in a variety of formats. These may include e.g., \"June 23, 1912\" or \"5/11/1930\" (month, day, year). But not all values are valid dates. Invalid dates may include e.g., \"June 2018\", \"3/06\" (incomplete dates) or even arbitrary natural language. Add a start_date_description field adjacent to the start_date column to enable filtering invalid date values for analysts. Normalize all valid date values in start_date to ISO 8601 (i.e., YYYY-MM-DD).\n",
    "\n",
    "Your script should take `data.csv` as input and produce a cleansed `enriched.csv` and `enriched.snappy.parquet` files according to the step requirements above.\n",
    "\n",
    "## Submission Guidelines\n",
    "We ask that your solutions be implemented in Python (3.8 or newer) or PySpark (3.3 or newer). If you would like to present skills for both approach, feel free to prepare two separate jupyter notebooks. Assume that code will be used monthly to process the data and store it in AWS S3 based data lake. With that assumption please prepare for discussion how this code can be scheduled and how outputs should be stored in S3 bucket.\n",
    "\n",
    "### Assessment Criteria\n",
    "Our goal is not to fool you. On the contrary, we would like to see you in your best light! We value clean, DRY and documented code; and in the interest of full disclosure, our assessment criteria is outlined below (in order of significance):\n",
    "\n",
    "1. Your ability to effectively solve the problems posed.\n",
    "1. Your ability to solve these problems in a clear and logical manner, with tasteful design.\n",
    "1. Your ability to appropriately document and comment your code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57856e32638af754",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Reading a file line by line using csv.reader\n",
    "The simplest solution. Generally effective for smaller to medium-sized files when we don't need to load the entire file into memory at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5b54becf929ac",
   "metadata": {},
   "source": [
    "#### String cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ba79967bf1157ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T22:48:52.607995Z",
     "start_time": "2026-02-09T22:48:52.579894Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "try:\n",
    "    with (\n",
    "        open(\"./data/data.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as infile,\n",
    "        open(\"./output/data_1.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as outfile\n",
    "        ):\n",
    "\n",
    "\n",
    "        reader = csv.reader(infile, delimiter=\",\")\n",
    "        writer = csv.writer(outfile, delimiter=\",\")\n",
    "\n",
    "        header = next(reader)\n",
    "        writer.writerow(header)\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) >= 9 and row[8]:  # Ensure there are at least 9 columns and the 9th is not empty\n",
    "                row[8] = \" \".join(row[8].split())  # Normalize spaces in the 9th column\n",
    "            writer.writerow(row)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c9eb6f6ed1980",
   "metadata": {},
   "source": [
    "#### Code swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdd9ad40d09ef343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T21:38:17.596217Z",
     "start_time": "2026-02-06T21:38:17.530215Z"
    }
   },
   "outputs": [],
   "source": [
    " import csv\n",
    "\n",
    "# from pathlib import Path\n",
    "\n",
    "# Load state abbreviations into a dictionary\n",
    "states = {}\n",
    "with open(\"./data/state_abbreviations.csv\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        code = row[\"state_abbr\"].strip()\n",
    "        name = row[\"state_name\"].strip()\n",
    "        states[code] = name\n",
    "\n",
    "\n",
    "# Process main data file\n",
    "with open(\"./output/data_1.csv\", \"r\", encoding=\"utf-8\", newline=\"\") as infile, \\\n",
    "     open(\"./output/data_2.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "    writer = csv.DictWriter(outfile, fieldnames=reader.fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    missing_states = set()\n",
    "\n",
    "    for row in reader:\n",
    "        state_code = row.get(\"state\")\n",
    "\n",
    "        if state_code:\n",
    "            state_code = state_code.strip()\n",
    "            if state_code in states:\n",
    "                row[\"state\"] = states[state_code]\n",
    "            else:\n",
    "                missing_states.add(state_code)\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "    if missing_states:\n",
    "        print(\"Missing state codes:\", \", \".join(sorted(missing_states)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d840e75686e09fb6",
   "metadata": {},
   "source": [
    "#### Date offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530f7ba8b25877a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T22:29:56.554118Z",
     "start_time": "2026-02-06T22:29:56.499237Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from dateutil import parser\n",
    "\n",
    "def date_validate(date_str):\n",
    "    #\"\"\"Validate and change format to ISO 8601.\"\"\"\n",
    "    try:\n",
    "        # parse date\n",
    "        parsed_date = parser.parse(date_str, yearfirst=True)\n",
    "\n",
    "        # return date in format YYYY-MM-DD\n",
    "        return parsed_date.strftime('%Y-%m-%d'), \"Valid date\"\n",
    "    except (ValueError, TypeError):\n",
    "        # If date is invalid - desc - invalid\n",
    "        return \"\", \"Invalid date\"\n",
    "\n",
    "input_path = \"./output/data_2.csv\"\n",
    "output_path = \"./output/data_3.csv\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\", newline=\"\") as infile, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "\n",
    "    reader = csv.DictReader(infile)\n",
    "\n",
    "    # add new column to existing fieldnames\n",
    "    fieldnames = reader.fieldnames + [\"start_date_description\"]\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "\n",
    "    for row in reader:\n",
    "        date_str = row.get(\"start_date\")  # change if column name differs\n",
    "\n",
    "        formatted_date, description = date_validate(date_str)\n",
    "        row[\"start_date\"] = formatted_date\n",
    "        row[\"start_date_description\"] = description\n",
    "\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31635328bb0718a1",
   "metadata": {},
   "source": [
    " #### Create enriched.csv and enriched.snappy.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf74c89-dcc0-466e-853a-87f7daf5c9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/gosia/Library/Python/3.9/lib/python/site-packages (2.3.3)\n",
      "Requirement already satisfied: pyarrow in /Users/gosia/Library/Python/3.9/lib/python/site-packages (21.0.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/gosia/Library/Python/3.9/lib/python/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gosia/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gosia/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gosia/Library/Python/3.9/lib/python/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U pandas pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49c13b3a4d35fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T16:13:47.417207Z",
     "start_time": "2026-02-09T16:13:47.352039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrichment complete! CSV and Parquet files created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "\n",
    "STATE_ABBREVIATIONS_PATH = \"./data/state_abbreviations.csv\"\n",
    "INPUT_PATH = \"./data/data.csv\"\n",
    "OUTPUT_CSV_PATH = \"./output/enriched.csv\"\n",
    "OUTPUT_PARQUET_PATH = \"./output/enriched.snappy.parquet\"\n",
    "\n",
    "STATE_COLUMN = \"state\"\n",
    "START_DATE_COLUMN = \"start_date\"\n",
    "START_DATE_DESC_COLUMN = \"start_date_description\"\n",
    "\n",
    "CLEAN_TEXT_COLUMN = \"bio\"\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def date_validate(date_str):\n",
    "    \"\"\"Validate a date string and return ISO 8601 format + description.\"\"\"\n",
    "    if not date_str:\n",
    "        return \"\", \"Missing date\"\n",
    "\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_str, yearfirst=True)\n",
    "        return parsed_date.strftime(\"%Y-%m-%d\"), \"Valid date\"\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\", \"Invalid date\"\n",
    "\n",
    "\n",
    "def load_state_lookup(path):\n",
    "    \"\"\"Load state abbreviations into a dictionary.\"\"\"\n",
    "    states = {}\n",
    "    with open(path, encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            code = row[\"state_abbr\"].strip()\n",
    "            name = row[\"state_name\"].strip()\n",
    "            states[code] = name\n",
    "    return states\n",
    "\n",
    "\n",
    "def normalize_whitespace(value):\n",
    "    \"\"\"Normalize internal whitespace in a string.\"\"\"\n",
    "    return \" \".join(value.split())\n",
    "\n",
    "\n",
    "# -------------------- Main Processing --------------------\n",
    "\n",
    "def enrich_csv(input_path, output_csv_path, output_parquet_path, state_lookup):\n",
    "    missing_states = set()\n",
    "    enriched_rows = []\n",
    "\n",
    "    with open(input_path, encoding=\"utf-8\", newline=\"\") as infile, \\\n",
    "         open(output_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + [START_DATE_DESC_COLUMN]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for row in reader:\n",
    "            # Optional string cleaning\n",
    "            if CLEAN_TEXT_COLUMN and row.get(CLEAN_TEXT_COLUMN):\n",
    "                row[CLEAN_TEXT_COLUMN] = normalize_whitespace(row[CLEAN_TEXT_COLUMN])\n",
    "\n",
    "            # State code → full name\n",
    "            state_code = row.get(STATE_COLUMN)\n",
    "            if state_code:\n",
    "                state_code = state_code.strip()\n",
    "                if state_code in state_lookup:\n",
    "                    row[STATE_COLUMN] = state_lookup[state_code]\n",
    "                else:\n",
    "                    missing_states.add(state_code)\n",
    "\n",
    "            # Date validation / normalization\n",
    "            formatted_date, description = date_validate(row.get(START_DATE_COLUMN))\n",
    "            row[START_DATE_COLUMN] = formatted_date\n",
    "            row[START_DATE_DESC_COLUMN] = description\n",
    "\n",
    "            writer.writerow(row)\n",
    "            enriched_rows.append(row)  # store for parquet\n",
    "\n",
    "\n",
    "    # Create Parquet with Snappy\n",
    "    # It needs to switch to pandas - the standard csv module doesn’t handle Parquet.\n",
    "    if enriched_rows:\n",
    "        df = pd.DataFrame(enriched_rows)\n",
    "        df.to_parquet(output_parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "\n",
    "    if missing_states:\n",
    "        print(\"Missing state codes:\", \", \".join(sorted(missing_states)))\n",
    "\n",
    "\n",
    "# -------------------- Entry Point --------------------\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        state_lookup = load_state_lookup(STATE_ABBREVIATIONS_PATH)\n",
    "        enrich_csv(INPUT_PATH, OUTPUT_CSV_PATH, OUTPUT_PARQUET_PATH, state_lookup)\n",
    "        print(\"Enrichment complete! CSV and Parquet files created.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88541c56c0d1a05e",
   "metadata": {},
   "source": [
    "\n",
    "#### Using PANDAS\n",
    "\n",
    "Here’s a chunked pandas version:\n",
    "- the same logic as above\n",
    "- to optimize this for memory efficiency (especially with large files)\n",
    "- instead of collecting all rows into a list, we process and write each chunk to Parquet as we go.\n",
    "- this will reduce memory usage since we’re not holding everything in memory at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335e3488833e4fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T22:52:28.344463Z",
     "start_time": "2026-02-09T22:52:28.253487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrichment complete! CSV and Parquet files created.\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U pandas pyarrow\n",
    "import csv\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "\n",
    "# -------------------- Configuration --------------------\n",
    "\n",
    "STATE_ABBREVIATIONS_PATH = \"./data/state_abbreviations.csv\"\n",
    "INPUT_PATH = \"./data/data.csv\"\n",
    "OUTPUT_CSV_PATH = \"./output/enriched_1.csv\"\n",
    "OUTPUT_PARQUET_PATH = \"./output/enriched_1.snappy.parquet\"\n",
    "\n",
    "STATE_COLUMN = \"state\"\n",
    "START_DATE_COLUMN = \"start_date\"\n",
    "START_DATE_DESC_COLUMN = \"start_date_description\"\n",
    "\n",
    "CLEAN_TEXT_COLUMN = \"bio\"\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "\n",
    "def date_validate(date_str):\n",
    "    \"\"\"Validate a date string and return ISO 8601 format + description.\"\"\"\n",
    "    if not date_str:\n",
    "        return \"\", \"Missing date\"\n",
    "\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_str, yearfirst=True)\n",
    "        return parsed_date.strftime(\"%Y-%m-%d\"), \"Valid date\"\n",
    "    except (ValueError, TypeError):\n",
    "        return \"\", \"Invalid date\"\n",
    "\n",
    "\n",
    "def load_state_lookup(path):\n",
    "    \"\"\"Load state abbreviations into a dictionary.\"\"\"\n",
    "    states = {}\n",
    "    with open(path, encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            code = row[\"state_abbr\"].strip()\n",
    "            name = row[\"state_name\"].strip()\n",
    "            states[code] = name\n",
    "    return states\n",
    "\n",
    "\n",
    "def normalize_whitespace(value):\n",
    "    \"\"\"Normalize internal whitespace in a string.\"\"\"\n",
    "    return \" \".join(value.split())\n",
    "\n",
    "\n",
    "# -------------------- Main Processing --------------------\n",
    "\n",
    "def enrich_csv(input_path, output_csv_path, output_parquet_path, state_lookup):\n",
    "    missing_states = set()\n",
    "\n",
    "    # Open CSV for writing\n",
    "    with open(input_path, encoding=\"utf-8\", newline=\"\") as infile, \\\n",
    "         open(output_csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as outfile:\n",
    "\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + [START_DATE_DESC_COLUMN]\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Temporary DataFrame to accumulate rows for Parquet in chunks\n",
    "        chunk_rows = []\n",
    "\n",
    "        # Process each row\n",
    "        for row in reader:\n",
    "            # Optional string cleaning\n",
    "            if CLEAN_TEXT_COLUMN and row.get(CLEAN_TEXT_COLUMN):\n",
    "                row[CLEAN_TEXT_COLUMN] = normalize_whitespace(row[CLEAN_TEXT_COLUMN])\n",
    "\n",
    "            # State code → full name\n",
    "            state_code = row.get(STATE_COLUMN)\n",
    "            if state_code:\n",
    "                state_code = state_code.strip()\n",
    "                if state_code in state_lookup:\n",
    "                    row[STATE_COLUMN] = state_lookup[state_code]\n",
    "                else:\n",
    "                    missing_states.add(state_code)\n",
    "\n",
    "            # Date validation / normalization\n",
    "            formatted_date, description = date_validate(row.get(START_DATE_COLUMN))\n",
    "            row[START_DATE_COLUMN] = formatted_date\n",
    "            row[START_DATE_DESC_COLUMN] = description\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Collect row for Parquet\n",
    "            chunk_rows.append(row)\n",
    "\n",
    "            # Every 1000 rows, write to Parquet\n",
    "            if len(chunk_rows) >= 1000:\n",
    "                df_chunk = pd.DataFrame(chunk_rows)\n",
    "                df_chunk.to_parquet(output_parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "                chunk_rows = []  # Clear the list after writing chunk\n",
    "\n",
    "        # Write any remaining rows to Parquet\n",
    "        if chunk_rows:\n",
    "            df_chunk = pd.DataFrame(chunk_rows)\n",
    "            df_chunk.to_parquet(output_parquet_path, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "\n",
    "    if missing_states:\n",
    "        print(\"Missing state codes:\", \", \".join(sorted(missing_states)))\n",
    "\n",
    "\n",
    "# -------------------- Entry Point --------------------\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        state_lookup = load_state_lookup(STATE_ABBREVIATIONS_PATH)\n",
    "        enrich_csv(INPUT_PATH, OUTPUT_CSV_PATH, OUTPUT_PARQUET_PATH, state_lookup)\n",
    "        print(\"Enrichment complete! CSV and Parquet files created.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9112b076-37b9-49ba-995d-ac5b29f0aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enriched_1.snappy.parquet - schema\n",
      "\n",
      "{\"Tag\":\"name=schema, inname=Schema\",\"Fields\":[{\"Tag\":\"name=name, inname=Name, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=gender, inname=Gender, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=birthdate, inname=Birthdate, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=address, inname=Address, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=city, inname=City, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=state, inname=State, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=zipcode, inname=Zipcode, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=email, inname=Email, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=bio, inname=Bio, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=job, inname=Job, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=start_date, inname=Start_date, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"},{\"Tag\":\"name=start_date_description, inname=Start_date_description, type=BYTE_ARRAY, convertedtype=UTF8, logicaltype=STRING, repetitiontype=OPTIONAL, encoding=RLE_DICTIONARY\"}]}\n",
      "\n",
      "\n",
      "\n",
      "enriched_1.snappy.parquet - row count\n",
      "\n",
      "500\n",
      "\n",
      "\n",
      "\n",
      "enriched.snappy.parquet - row count\n",
      "\n",
      "500\n",
      "\n",
      "\n",
      "\n",
      "enriched.csv - row count\n",
      "\n",
      "     501\n",
      "\n",
      "\n",
      "\n",
      "enriched.csv - head -3\n",
      "\n",
      "name,gender,birthdate,address,city,state,zipcode,email,bio,job,start_date,start_date_description\n",
      "Leslee Corwin,M,1974-02-01,4933 Weber Walks,Lake Carey,Kansas,32725,hansen.kennedy@yahoo.com,At aut velit unde minus recusandae molestias. Est maxime labore nostrum. Vero debitis neque doloremque accusantium incidunt corporis et et.,Education administrator,2026-10-06,Valid date\n",
      "Orris Kuvalis,M,1997-01-08,092 Kanye Forge,South Doshiamouth,Tennessee,08955,nicky.brown@yahoo.com,Corporis non harum doloribus ab provident. Alias autem error id modi saepe. Ut delectus fugit dolores.,Industrial buyer,,Invalid date\n",
      "\n",
      "\n",
      "\n",
      "data.csv - head -3\n",
      "\n",
      "name,gender,birthdate,address,city,state,zipcode,email,bio,job,start_date\n",
      "Leslee Corwin,M,1974-02-01,4933 Weber Walks,Lake Carey,KS,32725,hansen.kennedy@yahoo.com,At aut velit unde minus recusandae molestias. Est maxime labore nostrum.\t Vero debitis neque doloremque accusantium incidunt corporis et et.,Education administrator,10/06\n",
      "Orris Kuvalis,M,1997-01-08,092 Kanye Forge,South Doshiamouth,TN,08955,nicky.brown@yahoo.com,\"Corporis non harum doloribus ab provident.\t Alias autem error id modi saepe. Ut delectus fugit dolores.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"enriched_1.snappy.parquet - schema\\n\")\n",
    "!parquet-tools schema ./output/enriched_1.snappy.parquet\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"enriched_1.snappy.parquet - row count\\n\")\n",
    "!parquet-tools row-count ./output/enriched_1.snappy.parquet\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"enriched.snappy.parquet - row count\\n\")\n",
    "!parquet-tools row-count ./output/enriched.snappy.parquet\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"enriched.csv - row count\\n\")\n",
    "!cat ./output/enriched.csv | wc -l\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"enriched.csv - head -3\\n\")\n",
    "!head -3 ./output/enriched.csv\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"data.csv - head -3\\n\")\n",
    "!head -3 ./data/data.csv\n",
    "print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a93a36504dc87",
   "metadata": {},
   "source": [
    "#### Open points for discussion:\n",
    "1. AWS S3 Security and Access Control\n",
    "\n",
    "Implement robust IAM roles and policies to enforce least-privilege access to S3 buckets.\n",
    "\n",
    "Access should be clearly separated by environment, data layer, and user/service responsibilities.\n",
    "\n",
    "2. Data Partitioning Strategy\n",
    "\n",
    "Partition data based on ingestion (lake arrival) timestamp, typically at a daily granularity.\n",
    "\n",
    "Follow Hive-compatible directory conventions: _<bucket>/<layer>/year/month/day/_\n",
    "\n",
    "This enables efficient query pruning and scalable data organization.\n",
    "\n",
    "3. Data Layering\n",
    "\n",
    "Adopt a multi-layer data lake architecture:\n",
    "* Raw layer: immutable source data as ingested.\n",
    "* Curated layer: transformed and standardized data\n",
    "\n",
    "Curated datasets should retain the same file naming and structural alignment as raw data where feasible to support traceability and simplify debugging and issue resolution.\n",
    "\n",
    "4. Monitoring and Error Handling\n",
    "\n",
    "Implement end-to-end monitoring to validate that all expected files are processed successfully.\n",
    "\n",
    "Define clear error-handling mechanisms for failed processing, including logging, retry strategies, quarantine locations, and alerting.\n",
    "\n",
    "5. Data Reconciliation\n",
    "\n",
    "Perform reconciliation checks between source and target datasets, including file counts and row-level comparisons, to ensure data completeness and consistency.\n",
    "\n",
    "6. Processing Models: Batch vs. Streaming\n",
    "\n",
    "Select processing approaches based on latency, scalability, and operational requirements:\n",
    "* Scheduled batch processing via notebooks or jobs (e.g., Athena or SageMaker), triggered using cron-based schedules.\n",
    "* Time-based container execution using ECS tasks on AWS Fargate (serverless compute) or equivalent platforms such as Google Cloud Run to avoid Kubernetes overhead.\n",
    "* Event-driven processing using AWS Lambda triggered by S3 object creation events (via EventBridge) to support near-real-time ingestion.\n",
    "\n",
    "7. Consider using PySpark instead of pandas\n",
    "8. Evaluate a more suitable file format—most likely Parquet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
